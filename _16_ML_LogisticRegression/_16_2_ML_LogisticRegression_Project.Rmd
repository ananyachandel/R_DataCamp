---
title: "ML Logistic Regression project"
output: html_notebook
---

#Logistic Regression Project

In this project we will be working with the UCI adult dataset. We will be attempting to predict if people in the data set belong in a certain class by salary, either making <=50k or >50k per year.
Typically most of your time is spent cleaning data, not running the few lines of code that build your model, this project will try to reflect that by showing different issues that may arise when cleaning data.

## Get the Data

### Read in the adult_sal.csv file and set it to a data frame called adult.
```{r}
remove(list = ls())
adult <- read.csv("/Users/sobil/Documents/Certifications/DataScienceAndMachineLearningBootcampWithRUdemy/Notes/Training Exercises/Machine Learning Projects/CSV files for ML Projects/adult_sal.csv")
```
### Check the head of adult
```{r}
head(adult)
```
### You should notice the index has been repeated. Drop this column.
```{r}
adult <- adult[,c(-1)]
```
### Check the head,str, and summary of the data now.
```{r}
head(adult)
```
```{r}
str(adult)
```
```{r}
summary(adult)
```


## Data Cleaning
Notice that we have a lot of columns that are cateogrical factors, however a lot of these columns have too many factors than may be necessary. In this data cleaning section we'll try to clean these columns up by reducing the number of factors.

### type_employer column

### Use table() to check out the frequency of the type_employer column.
```{r}
table(adult$type_employer)
```
### How many Null values are there for type_employer? What are the two smallest groups?
Null = 1836
2 smallest group = Never-worked and Without-pay

### Combine these two smallest groups into a single group called "Unemployed". There are lots of ways to do this, so feel free to get creative. Hint: It may be helpful to convert these objects into character data types (as.character() and then use sapply with a custom function)
```{r}
unemplyed <- function(emp) {
  emp <- as.character(emp)
  if(emp == 'Never-worked' || emp == 'Without-pay'){
    emp <- "Unemployed"
  }
  return(emp)
}
adult$type_employer <- sapply(adult$type_employer, FUN = unemplyed)
table(adult$type_employer)
```
### What other columns are suitable for combining? Combine State and Local gov jobs into a category called SL-gov and combine self-employed jobs into a category called self-emp.
```{r}
job <- function(emp){
  emp <- as.character(emp)
  if(emp == 'Local-gov' || emp == 'State-gov'){
    emp <- "SL-gov"
  } else if(emp == 'Self-emp-inc' || emp == 'Self-emp-not-inc') {
    emp <- "self-emp"
  }
  return(emp)
}
adult$type_employer <- sapply(adult$type_employer, FUN = job)
table(adult$type_employer)
```
## Marital Column

### Use table() to look at the marital column
```{r}
table(adult$marital)
```
### Reduce this to three groups:

Married
Not-Married
Never-Married
```{r}
group_marital <- function(mar){
    mar <- as.character(mar)
    
    # Not-Married
    if (mar=='Separated' | mar=='Divorced' | mar=='Widowed'){
        return('Not-Married')
    
    # Never-Married   
    }else if(mar=='Never-married'){
        return(mar)
    
     #Married
    }else{
        return('Married')
    }
}
adult$marital <- sapply(adult$marital,group_marital)
table(adult$marital)
```
## Country Column

### Check the country column using table()
```{r}
table(adult$country)
```
### Group these countries together however you see fit. You have flexibility here because there is no right/wrong way to do this, possibly group by continents. You should be able to reduce the number of groups here significantly though.
```{r}
levels(adult$country)
```
```{r}
Asia <- c('China','Hong','India','Iran','Cambodia','Japan', 'Laos' ,
          'Philippines' ,'Vietnam' ,'Taiwan', 'Thailand')

North.America <- c('Canada','United-States','Puerto-Rico' )

Europe <- c('England' ,'France', 'Germany' ,'Greece','Holand-Netherlands','Hungary',
            'Ireland','Italy','Poland','Portugal','Scotland','Yugoslavia')

Latin.and.South.America <- c('Columbia','Cuba','Dominican-Republic','Ecuador',
                             'El-Salvador','Guatemala','Haiti','Honduras',
                             'Mexico','Nicaragua','Outlying-US(Guam-USVI-etc)','Peru',
                            'Jamaica','Trinadad&Tobago')
Other <- c('South')
```

```{r}
group_country <- function(ctry){
    if (ctry %in% Asia){
        return('Asia')
    }else if (ctry %in% North.America){
        return('North.America')
    }else if (ctry %in% Europe){
        return('Europe')
    }else if (ctry %in% Latin.and.South.America){
        return('Latin.and.South.America')
    }else{
        return('Other')      
    }
}
```

```{r}
adult$country <- sapply(adult$country,group_country)
```
### Use table() to confirm the groupings
```{r}
table(adult$country)
```
### Check the str() of adult again. Make sure any of the columns we changed have factor levels with factor()
```{r}
str(adult)
```
```{r}
adult$type_employer <- sapply(adult$type_employer,factor)
adult$country <- sapply(adult$country,factor)
adult$marital <- sapply(adult$marital,factor)
str(adult)
```
### We could still play around with education and occupation to try to reduce the number of factors for those columns, but let's go ahead and move on to dealing with the missing data. Feel free to group thos columns as well and see how they effect your model.

# Missing Data

### Notice how we have data that is missing.

## Amelia

### Install and load the Amelia package.
```{r}
library(Amelia)
```
### Convert any cell with a '?' or a ' ?' value to a NA value.
```{r}
adult[adult == '?'] <- NA
```
## Using table() on a column with NA values should now not display those NA values, instead you'll just see 0 for ?. Optional: Refactor these columns (may take awhile). For example:
```{r}
table(adult$type_employer)
```
```{r}
adult$type_employer <- sapply(adult$type_employer,factor)
adult$country <- sapply(adult$country,factor)
adult$marital <- sapply(adult$marital,factor)
adult$occupation <- sapply(adult$occupation,factor)
```

### Play around with the missmap function from the Amelia package. Can you figure out what its doing and how to use it?
```{r}
missmap(adult)
```
```{r}
missmap(adult,y.at=c(1),y.labels = c(''),col=c('yellow','black'))
```
### Use na.omit() to omit NA data from the adult data frame. Note, it really depends on the situation and your data to judge whether or not this is a good decision. You shouldn't always just drop NA values.
```{r}
adult <- na.omit(adult)
```
### Use missmap() to check that all the NA values were in fact dropped.
```{r}
missmap(adult,y.at=c(1),y.labels = c(''),col=c('yellow','black'))
```
# EDA

Although we've cleaned the data, we still have explored it using visualization.
### Check the str() of the data.
```{r}
str(adult)
```
### Use ggplot2 to create a histogram of ages, colored by income.
```{r}
library(ggplot2)
library(dplyr)
ggplot(adult,aes(age)) + geom_histogram(aes(fill=income),color='black',binwidth=1) + theme_bw()
```
### Plot a histogram of hours worked per week
```{r}
ggplot(adult,aes(hr_per_week)) + geom_histogram() + theme_bw()
```
### Rename the country column to region column to better reflect the factor levels.
```{r}
names(adult)[names(adult)=="country"] <- "region"
str(adult)
```
### Create a barplot of region with the fill color defined by income class. Optional: Figure out how rotate the x axis text for readability
```{r}
ggplot(adult,aes(region)) + geom_bar(aes(fill=income),color='black')+theme_bw()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
# Building a Model

Now it's time to build a model to classify people into two groups: Above or Below 50k in Salary.
## Logistic Regression

Refer to the Lecture or ISLR if you are fuzzy on any of this.
Logistic Regression is a type of classification model. In classification models, we attempt to predict the outcome of categorical dependent variables, using one or more independent variables. The independent variables can be either categorical or numerical.
Logistic regression is based on the logistic function, which always takes values between 0 and 1. Replacing the dependent variable of the logistic function with a linear combination of dependent variables we intend to use for regression, we arrive at the formula for logistic regression.
### Take a quick look at the head() of adult to make sure we have a good overview before going into building the model!
```{r}
head(adult)
```
## Train Test Split

### Split the data into a train and test set using the caTools library as done in previous lectures. Reference previous solutions notebooks if you need a refresher.
```{r}
# Import Library
library(caTools)

# Set a random see so your "random" results are the same as this notebook
set.seed(101) 

# Split up the sample, basically randomly assigns a booleans to a new column "sample"
sample <- sample.split(adult$income, SplitRatio = 0.70) # SplitRatio = percent of sample==TRUE

# Training Data
train = subset(adult, sample == TRUE)

# Testing Data
test = subset(adult, sample == FALSE)
```

## Training the Model

### Use all the features to train a glm() model on the training data set, pass the argument family=binomial(logit) into the glm function.
```{r}
model = glm(income ~ ., family = binomial(logit), data = train)
```
If you get a warning, this just means that the model may have guessed the probability of a class with a 0% or 100% chance of occuring.
### Check the model summary
```{r}
summary(model)
```
### We have still a lot of features! Some important, some not so much. R comes with an awesome function called step(). The step() function iteratively tries to remove predictor variables from the model in an attempt to delete variables that do not significantly add to the fit. How does it do this? It uses AIC. Read the wikipedia page for AIC if you want to further understand this, you can also check out help(step). This level of statistics is outside the scope of this project assignment so let's keep moving along

### Use new.model <- step(your.model.name) to use the step() function to create a new model.
```{r}
new.model <- step(model)
```
### You should have noticed that the step() function kept all the features used previously! While we used the AIC criteria to compare models, there are other criteria we could have used. If you want you can try reading about the variable inflation factor (VIF) and vif() function to explore other options for comparison criteria. In the meantime let's continue on and see how well our model performed against the test set.

### Create a confusion matrix using the predict function with type='response' as an argument inside of that function.
```{r}
test$predicted.income = predict(model, newdata=test, type="response")

table(test$income, test$predicted.income > 0.5)
```
### You'll notice we have a rank deficient fit. Find out more about what issues this may cause by reading this stackexchange post.

### What was the accuracy of our model?
```{r}
(6372+1423)/(6372+1423+548+872)
```

### Calculate other measures of performance like, recall or precision.
```{r}
#recall
6732/(6372+548)
```
```{r}
#precision
6732/(6372+872)
```

